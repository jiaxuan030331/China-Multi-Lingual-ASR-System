"""
Workspace-optimized IntegratedASR: æ‰€æœ‰æ¨¡å‹ä¸‹è½½åˆ°/workspaceç›®å½•

æ ¸å¿ƒä¿®æ”¹ï¼š
1. è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•åˆ°/workspace
2. è¯­è¨€åˆ†ç±»å™¨æ¨¡å‹è‡ªåŠ¨ä¸‹è½½
3. ç¡®ä¿æ‰€æœ‰æ¨¡å‹å­˜å‚¨åœ¨/workspaceè€Œä¸æ˜¯rootç¼“å­˜
"""

import torch
import torch.nn as nn
import numpy as np
import time
import urllib.request
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Tuple, Dict, Any, Union, List
from dataclasses import dataclass
import logging
import ctranslate2

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©æ‰€æœ‰HuggingFaceæ¨¡å‹ä¸‹è½½åˆ°workspace
os.environ['HF_HOME'] = '/workspace/.cache/huggingface'
os.environ['TRANSFORMERS_CACHE'] = '/workspace/.cache/huggingface/transformers'
os.environ['HF_DATASETS_CACHE'] = '/workspace/.cache/huggingface/datasets'

# Import actual implementations  
from kimi_deployment.kimia_infer.api.kimia import KimiAudio
from WhisperLive.whisper_live.new_transcriber import WhisperModel
from kimi_deployment.kimia_infer.models.tokenizer.glm4_tokenizer import Glm4Tokenizer


@dataclass
class CTranslateFeatures:
    """CTranslate2æ ¼å¼çš„å…±äº«ç‰¹å¾"""
    audio_features: np.ndarray  # åŸå§‹éŸ³é¢‘ç‰¹å¾ (æ¢…å°”é¢‘è°±)
    encoder_output: ctranslate2.StorageView  # CTranslate2 encoderè¾“å‡º
    sequence_length: int
    language: Optional[str] = None
    confidence: Optional[float] = None
    glm4_tokens: Optional[torch.Tensor] = None


@dataclass 
class CTranslateResult:
    """CTranslate2å¤„ç†ç»“æœ"""
    text: str
    language: str
    confidence: float
    engine: str
    processing_time: float
    encoding_time: float
    language_detection_time: float
    decoding_time: float


def ensure_workspace_directory(subdir: str) -> str:
    """ç¡®ä¿workspaceå­ç›®å½•å­˜åœ¨"""
    workspace_dir = f"/workspace/{subdir}"
    os.makedirs(workspace_dir, exist_ok=True)
    return workspace_dir


def download_language_classifier():
    """ä¸‹è½½è¯­è¨€åˆ†ç±»å™¨æ¨¡å‹åˆ°workspace"""
    model_dir = ensure_workspace_directory("models")
    model_path = os.path.join(model_dir, "language_fnn_only2.pt")
    
    if os.path.exists(model_path):
        print(f"âœ… è¯­è¨€åˆ†ç±»å™¨å·²å­˜åœ¨: {model_path}")
        return model_path
    
    # è¿™é‡Œéœ€è¦æä¾›å®é™…çš„ä¸‹è½½URLï¼Œæš‚æ—¶åˆ›å»ºä¸€ä¸ªå ä½æ–‡ä»¶
    print(f"âš ï¸  è¯­è¨€åˆ†ç±»å™¨æ¨¡å‹ä¸å­˜åœ¨ï¼Œéœ€è¦ä»å®é™…ä½ç½®ä¸‹è½½åˆ°: {model_path}")
    print("ğŸ’¡ è¯·æä¾›è¯­è¨€åˆ†ç±»å™¨çš„ä¸‹è½½é“¾æ¥")
    
    # å¦‚æœæœ‰å®é™…çš„URLï¼Œå¯ä»¥è¿™æ ·ä¸‹è½½ï¼š
    # url = "https://example.com/language_fnn_only2.pt"
    # urllib.request.urlretrieve(url, model_path)
    
    return model_path


class WorkspaceEncoder:
    """
    Workspaceä¼˜åŒ–çš„CTranslate2 Whisper encoder
    ç¡®ä¿æ¨¡å‹ä¸‹è½½åˆ°workspaceç›®å½•
    """
    
    def __init__(self, whisper_model_path: str = "base"):
        self.whisper_model = None
        self.whisper_model_path = whisper_model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.download_root = ensure_workspace_directory("whisper_models")
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–CTranslate2 Whisperæ¨¡å‹ï¼Œä¸‹è½½åˆ°workspace"""
        try:
            print(f"ğŸ“¥ ä¸‹è½½/åŠ è½½Whisperæ¨¡å‹åˆ°: {self.download_root}")
            
            self.whisper_model = WhisperModel(
                model_size_or_path=self.whisper_model_path,
                device="cuda",
                compute_type="int8_float16",
                download_root=self.download_root,  # æŒ‡å®šä¸‹è½½ç›®å½•
                local_files_only=False  # å…è®¸ä¸‹è½½
            )
            print(f"âœ… Whisper encoderåŠ è½½æˆåŠŸ: {self.whisper_model_path}")
            return True
        except Exception as e:
            print(f"âŒ Whisper encoderåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def encode_audio(self, audio: np.ndarray) -> CTranslateFeatures:
        """ä½¿ç”¨CTranslate2 encoderè¿›è¡ŒéŸ³é¢‘ç¼–ç """
        if self.whisper_model is None:
            raise RuntimeError("CTranslate2 encoder not initialized")
        
        try:
            start_time = time.time()
            
            # 1. æå–éŸ³é¢‘ç‰¹å¾ (æ¢…å°”é¢‘è°±ç­‰)
            audio_features = self.whisper_model.feature_extractor(audio)
            
            # 2. ä½¿ç”¨CTranslate2 encoderè¿›è¡Œç¼–ç 
            encoder_output = self.whisper_model.encode(audio_features)
            
            encoding_time = time.time() - start_time
            
            # è·å–åºåˆ—é•¿åº¦
            encoder_output_cpu = encoder_output.to_device(ctranslate2.Device(0))  # CPU
            encoder_array = np.array(encoder_output_cpu)
            if encoder_array.dtype == object:
                encoder_array = np.stack(encoder_array)
            sequence_length = encoder_array.shape[1] if len(encoder_array.shape) > 1 else 1
            
            return CTranslateFeatures(
                audio_features=audio_features,
                encoder_output=encoder_output,
                sequence_length=sequence_length
            )
            
        except Exception as e:
            raise RuntimeError(f"CTranslate2 encoding failed: {e}")


class WorkspaceLanguageClassifier:
    """
    Workspaceä¼˜åŒ–çš„è¯­è¨€åˆ†ç±»å™¨
    è‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°workspace
    """
    
    def __init__(self, model_path: str = None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å¦‚æœæ²¡æœ‰æä¾›è·¯å¾„ï¼Œå°è¯•ä¸‹è½½åˆ°workspace
        if model_path is None:
            model_path = download_language_classifier()
        
        self.model_path = model_path
        
        # æ„å»ºåˆ†ç±»ç½‘ç»œ (ä¸è®­ç»ƒæ—¶ä¸€è‡´)
        self.classifier = nn.Sequential(
            nn.Linear(1280, 1280),
            nn.ReLU(),
            nn.LayerNorm(1280),
            nn.Dropout(0.1),
            nn.Linear(1280, 640),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(640, 3),
        ).to(self.device)
        
        # å°è¯•åŠ è½½è®­ç»ƒæƒé‡
        if os.path.exists(model_path):
            self.classifier.load_state_dict(
                torch.load(model_path, map_location=self.device)
            )
            self.classifier.eval()
            print(f"âœ… è¯­è¨€åˆ†ç±»å™¨åŠ è½½æˆåŠŸ: {model_path}")
        else:
            print(f"âš ï¸  è¯­è¨€åˆ†ç±»å™¨æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
            print("ğŸ’¡ å°†ä½¿ç”¨éšæœºæƒé‡ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
        
        # è¯­è¨€æ˜ å°„
        self.id2lang = {0: "zh", 1: "en", 2: "yue"}
    
    def predict_language(self, ctranslate_features: CTranslateFeatures) -> Tuple[str, float]:
        """åŸºäºCTranslate2 encoderè¾“å‡ºé¢„æµ‹è¯­è¨€"""
        try:
            start_time = time.time()
            
            encoder_output = ctranslate_features.encoder_output
            
            # 1. è½¬æ¢CTranslate2.StorageViewåˆ°numpy
            encoder_output_cpu = encoder_output.to_device(ctranslate2.Device(0))  # ç§»åˆ°CPU
            encoder_output_np = np.array(encoder_output_cpu)
            
            if encoder_output_np.dtype == object:
                encoder_output_np = np.stack(encoder_output_np)
            encoder_output_np = encoder_output_np.astype(np.float32)
            
            # 2. æ± åŒ–åˆ°å›ºå®šç»´åº¦ (B, H)
            pooled = encoder_output_np.mean(axis=1)  # shape: (1, H)
            pooled_tensor = torch.tensor(pooled, dtype=torch.float32, device=self.device)
            
            # 3. è¯­è¨€åˆ†ç±»æ¨ç†
            with torch.no_grad():
                logits = self.classifier(pooled_tensor)
                probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()
                
                pred_id = np.argmax(probs)
                confidence = float(probs[pred_id])
                language = self.id2lang[pred_id]
            
            detection_time = time.time() - start_time
            
            return language, confidence
            
        except Exception as e:
            # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            print(f"âš ï¸  è¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤: {e}")
            return "zh", 0.5


class WorkspaceKimiDecoder:
    """
    Workspaceä¼˜åŒ–çš„Kimiè§£ç å™¨
    ç¡®ä¿æ¨¡å‹ä¸‹è½½åˆ°workspace
    """
    
    def __init__(self, kimi_model_path: str = None, glm4_tokenizer_path: str = None):
        self.kimi_engine = None
        self.glm4_tokenizer = None
        self.kimi_model_path = kimi_model_path
        self.glm4_tokenizer_path = glm4_tokenizer_path or "THUDM/glm-4-voice-tokenizer"
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–Kimiå¼•æ“å’ŒGLM4 tokenizerï¼Œä¸‹è½½åˆ°workspace"""
        success = True
        
        # åˆå§‹åŒ–GLM4 tokenizer (ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°workspaceç¼“å­˜)
        if self.glm4_tokenizer_path:
            try:
                print(f"ğŸ“¥ ä¸‹è½½/åŠ è½½GLM4 tokenizer: {self.glm4_tokenizer_path}")
                self.glm4_tokenizer = Glm4Tokenizer(self.glm4_tokenizer_path)
                self.glm4_tokenizer = self.glm4_tokenizer.to(torch.device("cuda"))
                print(f"âœ… GLM4 tokenizeråŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ GLM4 tokenizeråŠ è½½å¤±è´¥: {e}")
                success = False
        
        # åˆå§‹åŒ–Kimiå¼•æ“ (å¦‚æœæä¾›äº†è·¯å¾„)
        if self.kimi_model_path:
            try:
                print(f"ğŸ“¥ åŠ è½½Kimiå¼•æ“: {self.kimi_model_path}")
                self.kimi_engine = KimiAudio(
                    model_path=self.kimi_model_path,
                    device="cuda",
                    torch_dtype="bfloat16",
                    load_detokenizer=False
                )
                print(f"âœ… Kimiå¼•æ“åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ Kimiå¼•æ“åŠ è½½å¤±è´¥: {e}")
                success = False
        else:
            print("ğŸ’¡ æœªæä¾›Kimiæ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡Kimiå¼•æ“åŠ è½½")
        
        return success
    
    def get_glm4_tokens(self, audio: np.ndarray) -> torch.Tensor:
        """è·å–GLM4 speech tokens"""
        if self.glm4_tokenizer is None:
            raise RuntimeError("GLM4 tokenizer not initialized")
        
        return self.glm4_tokenizer.tokenize(speech=audio, sr=16000)
    
    def decode_with_glm4_tokens(self, glm4_tokens: torch.Tensor) -> str:
        """ä½¿ç”¨GLM4 tokensè¿›è¡ŒKimiè§£ç """
        if self.kimi_engine is None:
            return "[Kimiå¼•æ“æœªåŠ è½½ï¼Œæ— æ³•è§£ç ]"
        
        try:
            # TODO: å®ç°å®é™…çš„Kimiè§£ç 
            return "[Kimi transcription with GLM4 tokens - to be implemented]"
        except Exception as e:
            return f"[Kimiè§£ç å¤±è´¥: {e}]"


class WorkspaceWhisperDecoder:
    """Workspaceä¼˜åŒ–çš„Whisperè§£ç å™¨"""
    
    def __init__(self, whisper_model):
        self.whisper_model = WhisperModel('large-3', device="cuda", compute_type="bfloat16")
        
    def decode_with_encoder_output(
        self, 
        ctranslate_features: CTranslateFeatures, 
        language: str = None
    ) -> str:
        """ä½¿ç”¨é¢„ç¼–ç çš„encoderè¾“å‡ºè¿›è¡ŒWhisperè§£ç """
        try:
            from WhisperLive.whisper_live.new_transcriber import TranscriptionOptions
            
            # åˆ›å»ºè½¬å†™é€‰é¡¹
            options = TranscriptionOptions(
                beam_size=5,
                best_of=5,
                patience=1.0,
                length_penalty=1.0,
                repetition_penalty=1.0,
                no_repeat_ngram_size=0,
                temperature=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
                compression_ratio_threshold=2.4,
                log_prob_threshold=-1.0,
                no_speech_threshold=0.6,
                condition_on_previous_text=True,
                prompt_reset_on_temperature=0.5,
                initial_prompt=None,
                prefix=None,
                suppress_blank=True,
                suppress_tokens=[-1],
                without_timestamps=False,
                max_initial_timestamp=1.0,
                word_timestamps=False,
                prepend_punctuations="\"'",
                append_punctuations="\"'",
                max_new_tokens=None,
                clip_timestamps="0",
                hallucination_silence_threshold=None
            )
            
            # è®¾ç½®è¯­è¨€
            if language and language in ["zh", "en", "yue"]:
                options.language = language
            
            # è·å–tokenizer
            tokenizer = self.whisper_model.hf_tokenizer
            
            # ä½¿ç”¨é¢„ç¼–ç çš„encoderè¾“å‡ºç”Ÿæˆè½¬å†™
            segments = list(self.whisper_model.generate_segments(
                features=ctranslate_features.audio_features,
                tokenizer=tokenizer,
                options=options,
                encoder_output=ctranslate_features.encoder_output  # å¤ç”¨é¢„ç¼–ç è¾“å‡ºï¼
            ))
            
            # åˆå¹¶æ‰€æœ‰æ®µè½
            text_segments = []
            for segment in segments:
                text_segments.append(segment.text)
            
            text = " ".join(text_segments).strip()
            return text
            
        except Exception as e:
            return f"[Whisperè§£ç å¤±è´¥: {e}]"


class SmartRouter:
    """æ™ºèƒ½è·¯ç”±å™¨"""
    
    def __init__(self, confidence_threshold: float = 0.7):
        self.confidence_threshold = confidence_threshold
    
    def route_decision(self, language: str, confidence: float, has_kimi: bool) -> str:
        """è·¯ç”±å†³ç­–"""
        if not has_kimi:
            return "whisper"
        
        if confidence < self.confidence_threshold:
            return "whisper"
        
        if language == "yue":  # ç²¤è¯­ç”¨Whisper
            return "whisper"
        
        if language in ["zh", "en"]:  # ä¸­è‹±æ–‡ç”¨Kimi
            return "kimi"
        
        return "whisper"


class WorkspaceIntegratedASR:
    """
    Workspaceä¼˜åŒ–çš„é›†æˆASRç³»ç»Ÿ
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. æ‰€æœ‰æ¨¡å‹ä¸‹è½½/ç¼“å­˜åˆ°/workspaceç›®å½•
    2. è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
    3. æ­£ç¡®ä½¿ç”¨CTranslate2æ¥å£
    4. æœ€å¤§åŒ–ç‰¹å¾å¤ç”¨
    """
    
    def __init__(
        self,
        whisper_model_path: str = "base",
        kimi_model_path: str = None,
        glm4_tokenizer_path: str = None,
        language_classifier_path: str = None,
        confidence_threshold: float = 0.7
    ):
        self.logger = logging.getLogger(__name__)
        
        # ç¡®ä¿workspaceç›®å½•å­˜åœ¨
        ensure_workspace_directory("models")
        ensure_workspace_directory("cache")
        
        # æ ¸å¿ƒç»„ä»¶
        self.
        self.language_classifier = WorkspaceLanguageClassifier(language_classifier_path)
        self.whisper_decoder = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.kimi_decoder = WorkspaceKimiDecoder(kimi_model_path, glm4_tokenizer_path)
        self.router = SmartRouter(confidence_threshold)
        
        self.is_initialized = False
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ï¼Œè‡ªåŠ¨ä¸‹è½½åˆ°workspace"""
        try:
            print("ğŸš€ åˆå§‹åŒ–Workspaceé›†æˆASRç³»ç»Ÿ...")
            print(f"ğŸ“‚ ç¼“å­˜ç›®å½•: {os.environ.get('HF_HOME', 'default')}")
            
            # 1. åˆå§‹åŒ–Whisper encoder
            print("\n1ï¸âƒ£ åˆå§‹åŒ–Whisper encoder...")
            if not self.encoder.initialize():
                print("âŒ Whisper encoderåˆå§‹åŒ–å¤±è´¥")
                return False
            
            # 2. åˆå§‹åŒ–Whisper decoder (å¤ç”¨encoderçš„æ¨¡å‹)
            print("\n2ï¸âƒ£ åˆå§‹åŒ–Whisper decoder...")
            self.whisper_decoder = WorkspaceWhisperDecoder(self.encoder.whisper_model)
            print("âœ… Whisper decoderåˆå§‹åŒ–æˆåŠŸ")
            
            # 3. åˆå§‹åŒ–è¯­è¨€åˆ†ç±»å™¨
            print("\n3ï¸âƒ£ è¯­è¨€åˆ†ç±»å™¨å·²åˆå§‹åŒ–")
            
            # 4. åˆå§‹åŒ–Kimiç›¸å…³ç»„ä»¶ (å¯é€‰)
            print("\n4ï¸âƒ£ åˆå§‹åŒ–Kimiç»„ä»¶...")
            kimi_success = self.kimi_decoder.initialize()
            if not kimi_success:
                print("âš ï¸  Kimiç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨Whisper")
            
            self.is_initialized = True
            print("\nğŸ‰ é›†æˆASRç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def transcribe(self, audio: np.ndarray) -> CTranslateResult:
        """è½¬å†™éŸ³é¢‘"""
        if not self.is_initialized:
            raise RuntimeError("System not initialized")
        
        start_time = time.time()
        
        try:
            # Step 1: ç¼–ç 
            print("ğŸ”„ æ­£åœ¨ç¼–ç éŸ³é¢‘...")
            encoding_start = time.time()
            ctranslate_features = self.encoder.encode_audio(audio)
            encoding_time = time.time() - encoding_start
            
            # Step 2: è¯­è¨€æ£€æµ‹
            print("ğŸ” æ­£åœ¨æ£€æµ‹è¯­è¨€...")
            detection_start = time.time()
            language, confidence = self.language_classifier.predict_language(ctranslate_features)
            ctranslate_features.language = language
            ctranslate_features.confidence = confidence
            detection_time = time.time() - detection_start
            
            # Step 3: è·¯ç”±å†³ç­–
            has_kimi = (self.kimi_decoder.kimi_engine is not None and 
                       self.kimi_decoder.glm4_tokenizer is not None)
            engine = self.router.route_decision(language, confidence, has_kimi)
            print(f"ğŸ¯ è·¯ç”±åˆ°: {engine}å¼•æ“")
            
            # Step 4: è§£ç 
            decoding_start = time.time()
            
            if engine == "kimi" and has_kimi:
                try:
                    # è·å–GLM4 tokens
                    if ctranslate_features.glm4_tokens is None:
                        ctranslate_features.glm4_tokens = self.kimi_decoder.get_glm4_tokens(audio)
                    
                    text = self.kimi_decoder.decode_with_glm4_tokens(ctranslate_features.glm4_tokens)
                except Exception as e:
                    print(f"âš ï¸  Kimiè§£ç å¤±è´¥ï¼Œå›é€€åˆ°Whisper: {e}")
                    text = self.whisper_decoder.decode_with_encoder_output(ctranslate_features, language)
                    engine = "whisper"
            else:
                text = self.whisper_decoder.decode_with_encoder_output(ctranslate_features, language)
            
            decoding_time = time.time() - decoding_start
            
            # ç»“æœ
            total_time = time.time() - start_time
            
            result = CTranslateResult(
                text=text,
                language=language,
                confidence=confidence,
                engine=engine,
                processing_time=total_time,
                encoding_time=encoding_time,
                language_detection_time=detection_time,
                decoding_time=decoding_time
            )
            
            print(f"âœ… è½¬å†™å®Œæˆ: '{text}'")
            return result
            
        except Exception as e:
            print(f"âŒ è½¬å†™å¤±è´¥: {e}")
            return CTranslateResult(
                text="",
                language="unknown",
                confidence=0.0,
                engine="error",
                processing_time=time.time() - start_time,
                encoding_time=0.0,
                language_detection_time=0.0,
                decoding_time=0.0
            )


