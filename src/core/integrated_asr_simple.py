"""
Simple Integrated ASR: å¹¶è¡Œå‡†å¤‡é˜¶æ®µï¼ŒFNNå†³ç­–åå•é“¾è§£ç 

æ ¸å¿ƒé€»è¾‘ï¼š
1. å¹¶è¡Œå¯åŠ¨ CT2é“¾(encode+LID) å’Œ Kimié“¾(tokenize)
2. ç­‰FNNå‡ºç»“æœï¼šzh/enâ†’ç»§ç»­Kimiï¼Œå…¶ä»–â†’ç»§ç»­CT2
3. å•é“¾è§£ç ï¼Œå¦ä¸€é“¾åœæ­¢
"""

import torch
import torch.nn as nn
import numpy as np
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dataclasses import dataclass
from typing import Optional, Tuple

# è®¾ç½®HuggingFaceç¼“å­˜åˆ°workspaceç›®å½•
os.environ['HF_HOME'] = '/workspace/.cache/huggingface'
os.environ['TRANSFORMERS_CACHE'] = '/workspace/.cache/huggingface/transformers'
os.environ['HF_DATASETS_CACHE'] = '/workspace/.cache/huggingface/datasets'

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs('/workspace/.cache/huggingface/hub', exist_ok=True)
os.makedirs('/workspace/.cache/huggingface/transformers', exist_ok=True)
os.makedirs('/workspace/.cache/huggingface/datasets', exist_ok=True)

# å¯¼å…¥å®é™…å®ç°
from kimi_deployment.kimia_infer.api.kimia import KimiAudio
from WhisperLive.whisper_live.new_transcriber import WhisperModel
from kimi_deployment.kimia_infer.models.tokenizer.glm4_tokenizer import Glm4Tokenizer
from kimi_deployment.app.load_model import load_kimi_model

@dataclass
class PrepareResult:
    """å‡†å¤‡é˜¶æ®µç»“æœ"""
    encoder_output: Optional[object] = None
    speech_tokens: Optional[torch.Tensor] = None
    language: Optional[str] = None
    confidence: Optional[float] = None


@dataclass
class TranscribeResult:
    """è½¬å†™ç»“æœ"""
    text: str
    language: str
    confidence: float
    engine: str
    total_time: float




class CT2Chain:
    """CTranslate2é“¾ï¼šç¼–ç +è¯­è¨€æ£€æµ‹"""
        
    def __init__(self, whisper_model_path: str = "large-v3", lid_model_path: str = None):
        """åˆå§‹åŒ–CT2æ¨¡å‹"""
        
        self.whisper_model = WhisperModel(
                model_size_or_path=whisper_model_path,
                device="cuda",
                compute_type="bfloat16",
                language_classifier_path=lid_model_path
            )
        print(f"âœ… CT2Chain initialized")
            
       
            
        self.stop_flag = threading.Event()
    
    def prepare(self, audio: np.ndarray) -> PrepareResult:
        """å‡†å¤‡é˜¶æ®µï¼šç¼–ç +è¯­è¨€æ£€æµ‹"""
        
        # CT2ç¼–ç 
        audio_features = self.whisper_model.feature_extractor(audio)
        encoder_output = self.whisper_model.encode(audio_features)
        
        # è¯­è¨€æ£€æµ‹
        language, confidence = self.whisper_model.custom_detect_language(encoder_output)
        
        return PrepareResult(
            encoder_output=encoder_output,
            language=language,
            confidence=confidence
        )
        
    def decode(self, encoder_output, language: str) -> str:
        """è§£ç é˜¶æ®µ"""
        if self.stop_flag.is_set():
            return ""
        
        try:
            from WhisperLive.whisper_live.new_transcriber import TranscriptionOptions
            
            options = TranscriptionOptions(
                beam_size=1,  # è´ªå¿ƒè§£ç ï¼Œæœ€å¿«
                best_of=1,
                temperature=[0.0],
                language=language if language in ["zh", "en", "yue"] else None,
                without_timestamps=True
            )
            
            tokenizer = self.whisper_model.hf_tokenizer
            segments = list(self.whisper_model.generate_segments(
                features=self.whisper_model.feature_extractor.audio,
                tokenizer=tokenizer,
                options=options,
                encoder_output=encoder_output
            ))
            
            text = " ".join([seg.text for seg in segments]).strip()
            return text
        except Exception as e:
            print(f"âŒ CT2 decode failed: {e}")
            return ""
    
    def stop(self):
        """åœæ­¢ä¿¡å·"""
        self.stop_flag.set()


class KimiChain:
    """Kimié“¾ï¼šGLM4 tokenizer + Kimi LLM"""
    
    def __init__(self, kimi_model_path_or_name: str ):#load glm4 tokenizer with kimi_audio class by default
        self.kimi_engine = None
        self.kimi_model_path = kimi_model_path_or_name
        self.kimi_engine = None
        self.kimi_engine = KimiAudio(
                    model_path_or_name=kimi_model_path_or_name,
                    device="cuda",
                    torch_dtype="bfloat16",
                    load_detokenizer=False  # èŠ‚çœæ˜¾å­˜
                )
        print(f"âœ… Kimi engine loaded")
            
        
        self.stop_flag = threading.Event()
    
    def prepare(self, audio: np.ndarray) -> PrepareResult:
        """å‡†å¤‡é˜¶æ®µï¼šGLM4 tokenize"""
        try:
            speech_tokens = self.glm4_tokenizer.tokenize(speech=audio, sr=16000)
            return PrepareResult(speech_tokens=speech_tokens)
        except Exception as e:
            print(f"âŒ Kimi prepare failed: {e}")
            return PrepareResult()
    
    def decode(self, speech_tokens: torch.Tensor) -> str:
        """è§£ç é˜¶æ®µ"""
        if self.stop_flag.is_set():
            return ""
        
        if self.kimi_engine is None:
            return "[Kimi engine not loaded]"
        
        try:
            # TODO: å®ç°çœŸæ­£çš„Kimiè§£ç 
            # result = self.kimi_engine.generate_from_speech_tokens(speech_tokens)
            return "[Kimi transcription - to be implemented]"
        except Exception as e:
            print(f"âŒ Kimi decode failed: {e}")
            return ""
    
    def stop(self):
        """åœæ­¢ä¿¡å·"""
        self.stop_flag.set()


class IntegratedASR:
    """é›†æˆASRç³»ç»Ÿï¼šå¹¶è¡Œå‡†å¤‡ï¼ŒFNNå†³ç­–ï¼Œå•é“¾è§£ç """
    
    def __init__(
        self,
        ct2_model_path = 'large-v3', #default to large-v3 for compatibility with ctranslate2
        kimi_model_path_or_name: str = "moonshotai/Kimi-Audio-7B-Instruct",
        lid_model_path: str = "/workspace/ASR/WhisperLive/language_fnn_only2.pt",
        confidence_threshold: float = 0.9
    ):
        self.kimi_chain = KimiChain(kimi_model_path_or_name)
        self.ct2_chain = CT2Chain(ct2_model_path,lid_model_path)
        
        self.confidence_threshold = confidence_threshold
        self.executor = ThreadPoolExecutor(max_workers=1) #only one worker for 40-50G GPU memory
        self.is_initialized = False
    

    
    def transcribe(self, audio: np.ndarray) -> TranscribeResult:
        """ä¸»è½¬å†™æ¥å£"""
        if not self.is_initialized:
            raise RuntimeError("System not initialized")
        
        start_time = time.time()
        
        # é‡ç½®åœæ­¢æ ‡å¿—
        self.ct2_chain.stop_flag.clear()
        self.kimi_chain.stop_flag.clear()
        
        try:
            print("ğŸ”„ å¹¶è¡Œå¯åŠ¨å‡†å¤‡é˜¶æ®µ...")
            
            # å¹¶è¡Œæäº¤ä»»åŠ¡
            future_ct2 = self.executor.submit(self.ct2_chain.prepare, audio, self.lid_fnn)
            future_kimi = self.executor.submit(self.kimi_chain.prepare, audio)
            
            # ç­‰å¾…CT2å®Œæˆï¼ˆåŒ…å«LIDï¼‰
            ct2_result = future_ct2.result()
            language = ct2_result.language
            confidence = ct2_result.confidence
            
            print(f"ğŸ” è¯­è¨€æ£€æµ‹: {language} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            # è·¯ç”±å†³ç­–
            if language in ("zh", "en") and confidence >= self.confidence_threshold and self.kimi_chain.glm4_tokenizer:
                # é€‰æ‹©Kimié“¾
                print("ğŸ¯ è·¯ç”±åˆ°Kimiå¼•æ“")
                self.ct2_chain.stop()
                
                kimi_result = future_kimi.result()
                if kimi_result.speech_tokens is not None:
                    text = self.kimi_chain.decode(kimi_result.speech_tokens)
                else:
                    text = "[Kimi tokenization failed]"
                engine = "kimi"
                
            else:
                # é€‰æ‹©CT2é“¾
                print("ğŸ¯ è·¯ç”±åˆ°Whisperå¼•æ“")
                self.kimi_chain.stop()
                
                text = self.ct2_chain.decode(ct2_result.encoder_output, language)
                engine = "whisper"
            
            total_time = time.time() - start_time
            
            result = TranscribeResult(
                text=text,
                language=language,
                confidence=confidence,
                engine=engine,
                total_time=total_time
            )
            
            print(f"âœ… è½¬å†™å®Œæˆ: '{text}' ({engine}, {total_time:.2f}s)")
            return result
            
        except Exception as e:
            print(f"âŒ è½¬å†™å¤±è´¥: {e}")
            return TranscribeResult(
                text="",
                language="unknown",
                confidence=0.0,
                engine="error",
                total_time=time.time() - start_time
            )
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
        torch.cuda.empty_cache()
        print("ğŸ”§ èµ„æºå·²æ¸…ç†")


